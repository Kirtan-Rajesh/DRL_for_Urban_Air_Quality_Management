

# ğŸ“Š DRL-Powered Urban Air Quality Management in Delhi (PPO-based)

---

## ğŸ“– Overview

Urban air pollution remains one of the most pressing environmental and public health crises globally, particularly in densely populated, traffic-intensive cities like **Delhi**. As industrial activity, vehicular emissions, and urban development increase, managing air quality becomes increasingly challenging. Traditional mitigation strategies, like fixed regulations or green infrastructure, often fall short because they lack adaptability to dynamic urban scenarios.

This project introduces a **Deep Reinforcement Learning (DRL)**-based framework to intelligently **optimize the placement of air purification booths** across Delhi. We use **Proximal Policy Optimization (PPO)** â€” a powerful reinforcement learning algorithm â€” to iteratively learn and identify high-impact booth locations, leveraging spatial, environmental, and demographic data.

---

## ğŸ¯ Objectives

The primary goals of this project include:

- ğŸ“Œ **Developing a multi-source data integration framework** using station-based AQI readings, live API data, and contextual urban information (population, traffic, green zones, etc.)
- ğŸ“Œ **Optimizing booth placements dynamically** based on air quality, traffic, and population density using PPO.
- ğŸ“Œ **Benchmarking RL-based placement strategies** against traditional random and greedy AQI-based approaches.
- ğŸ“Œ **Predicting pollution trends** using machine learning models to inform future booth deployments.
- ğŸ“Œ **Providing real-time, visual feedback and evaluation metrics** for urban planners and policymakers.

---

## ğŸ› ï¸ Methodology

The core components of this project can be broadly classified into these parts:

### ğŸ“¡ Data Collection & Preprocessing

- **Station-based AQI data**: Collected from Delhiâ€™s official government data repositories via [data.gov.in](https://data.gov.in)
- **Ozone3 API data**: Live, high-resolution AQI estimates to fill spatial gaps.
- **Custom spatial datasets**: Green spaces, traffic hotspots, population density, industrial zones (via OpenStreetMap and DDA datasets)

> Interpolations, median imputations, and normalization were applied to clean, scale, and spatially align data into a **multi-channel 50x50 grid**.

---

### ğŸ—ºï¸ Environment Design

The urban environment of Delhi is modeled as a **multi-channel grid** (50x50 cells), where each cell contains:
- ğŸ“Š AQI values
- ğŸ§‘â€ğŸ¤â€ğŸ§‘ Population density
- ğŸš¦ Traffic density
- ğŸ­ Industrial area influence
- ğŸŒ³ Green cover information
- ğŸ“ Existing booth presence

Each RL agent episode simulates **booth placements across this grid**, with the environment dynamically updating AQI values based on placements.

---

### ğŸ§  Agent Design: PPO with Spatial Attention

**DelhiAQIAgent** uses:
- **Convolutional layers** to process grid data
- **Spatial Attention layers** to focus on high-impact zones
- **Residual connections** for stability
- **Dual heads**:
  - **Policy head** for selecting actions
  - **Value head** for estimating expected rewards

We enhance PPO by:
- **Action masking** (invalid or unnecessary booth placements are masked)
- **Attention-weighted feature aggregation**
- **Multiple exploration strategies** (Boltzmann, UCB, High-AQI prioritization)

---

### ğŸ”„ PPO Training Process

**DelhiPPOTrainer** is responsible for:
- Initializing agent, memory, optimizer
- Collecting episode trajectories (state, action, reward, value, log-probability)
- Updating network using:
  - **Clipped objective**
  - **Generalized Advantage Estimation (GAE)**
  - **Dynamic entropy regularization**
- Tracking:
  - Total rewards
  - AQI improvements
  - Policy and value losses
  - Entropy trends
  - Custom environmental metrics (population, traffic, green policy violations)

---

### ğŸ“ˆ Validation and Testing

Validation involves:
- âœ… Ensuring proper action-space alignment
- âœ… Verifying AQI reductions after booth placements
- âœ… Inspecting action logits, value predictions, and entropy levels
- âœ… Confirming action validity masks work correctly

---

## ğŸ“Š Results

| Placement Strategy | Average AQI Improvement | Spatial Coverage | Policy Stability |
|:------------------|:------------------------|:----------------|:----------------|
| **Random**             | 3.2%                     | Low              | Stable           |
| **Greedy AQI**         | 7.1%                     | Medium           | Risk of redundancy|
| **RL PPO (Ours)**      | **13.5%**                | **High**         | **Stable**        |

The PPO-based agent achieved **over 13% AQI improvement**, with superior spatial distribution and minimal green space violations.

---

## ğŸ“Œ Key Features

- **Multi-source, real-time data fusion**
- **High-resolution spatial modeling**
- **Attention-augmented deep RL agent**
- **Dynamic entropy scheduling for stable exploration**
- **Flexible exploration strategies**
- **Extensive diagnostic metrics**
- **Comparative benchmarking**

---

## ğŸ” Why PPO?

Proximal Policy Optimization (PPO) is:
- **Stable**: Uses clipped objectives to avoid large policy updates.
- **Efficient**: Less sensitive to hyperparameters than alternatives like TRPO.
- **Adaptable**: Handles continuous, discrete, and multi-dimensional action spaces well.
- **Proven**: Successful in complex control tasks, robotics, and simulated environments.

In this work, PPO's stability and trust-region-like updates are crucial because:
- The environment is **non-stationary**
- Rewards (AQI improvements) are **delayed and sparse**
- The **state-action space is very high-dimensional**

---

## ğŸ“ Configuration

| Parameter         | Value      |
|:-----------------|:-----------|
| Learning Rate      | 2.5e-4     |
| Discount Factor Î³  | 0.97       |
| GAE Î»              | 0.95       |
| PPO Clip Ratio      | 0.15       |
| Entropy Coefficient | 0.1        |
| Max Grad Norm       | 1.0        |
| Batch Size          | 64         |
| Max Steps per Episode | 300      |
| Num Episodes        | 100        |

---

## ğŸ“š Dependencies

- Python 3.10+
- PyTorch
- Numpy
- Pandas
- Gymnasium
- Matplotlib
- TQDM
- IPython
- Scipy
- OpenStreetMap, DDA, Ozone3 API access

---

## ğŸ“Œ Future Work

- ğŸ” **Multi-agent RL models** â€” collaborating booths and distributed placement
- ğŸŒ **Real-time API integration** for continuous learning
- ğŸ™ï¸ **Deployable smart-city dashboards**
- ğŸ›ï¸ **Policy transfer learning** to other cities (Mumbai, Bangalore, Beijing)
- ğŸš¦ **Traffic-AQI interaction modeling**
- ğŸ® **Sim-to-real policy validation**
- ğŸ“± **Mobile/web-based air quality visualizations**

---

## ğŸ“ˆ Outcome Summary

âœ”ï¸ DRL-PPO framework dynamically optimizes urban AQI control  
âœ”ï¸ Significant improvements over static or heuristic strategies  
âœ”ï¸ Demonstrates real-world feasibility for **smart cities and environmental policy applications**

---

## ğŸ“Š Visualizations

- ğŸ“ˆ Training rewards and losses
- ğŸ“‰ AQI improvement over episodes
- ğŸŒ Heatmaps of AQI before and after booth placements
- ğŸ“Š Action selection distributions
- ğŸ“ Booth placement overlays on the city grid

---

## ğŸ“š References

The full list of 40+ references is available in the `paper` and included official datasets from:
- [data.gov.in](https://data.gov.in)
- Ozone3 AQI API
- OpenStreetMap (OSM)
- DDA open datasets

---

## ğŸ’¡ Conclusion

This project showcases how **AI-driven adaptive strategies can transform traditional urban management challenges**. The PPO-based DRL system outperforms static, rule-based heuristics by:
- Learning dynamically from real AQI, population, traffic, and land-use data  
- Adapting to complex, evolving environments  
- Supporting better-informed, scalable smart city air quality management  

